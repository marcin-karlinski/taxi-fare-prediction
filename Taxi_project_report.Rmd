---
title: "Predicting New York City taxi fare"
author: "Aleksandra Kowalewska, Marcin Karliński"
date: "02/2022"
output: 
  html_document:
    theme: paper
    toc: true
    toc_float:
      collapsed: true
      smooth_scroll: true
---

```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE)
opts_knit$set(root.dir = "C:/Users/marci/Desktop/UW/III Semester/ML2/RF_project")

options(scipen = 999)
Sys.setlocale("LC_ALL","English")
```

Libraries used in the project:
```{r libraries and options, warning = FALSE, message = FALSE}

#Data manipulation and vizualization 
library(data.table)
library(tidyverse)
library(viridis)
library(lubridate)
library(geosphere)
library(ggforce)
library(geojson)
library(leaflet)
library(leaflet.extras)

#linear regression
library(MASS)
library(caTools)
library(car)
library(quantmod)
library(MASS)
library(corrplot)
library(caret)
library(broom)

#regression tree
library(rpart)
library(rpart.plot)
library(rattle)

#bagged tree
library(e1071)      
library(ipred)  

#random Forest
library(randomForest)

#XGBoost
library(xgboost)
```

## Introduction

This project revolves around the prediction of a taxi fare in New York city. Our aim is to build a model which would forecast the fare of a taxi ride (tolls inclusive) given basic information, like pickup and dropoff location. Fee for a ride is dependent on a taxi price-list, however, it can vary depending on a route and traffic congestion. Our secondary aim was to compare how various machine learning models cope with the task of a regression. This basic information could later be used to build more advanced models for transportation companies for prediction of ride fares. 

## Data description

Dataset used in this project was made available in 2018 by Google for a Kaggle competition. It consists of 55m rows and variables depicting coordinates of pickup and drop-off, exact timestamp of a pickup, passenger count and total fare for the ride. 

Since the dataset is too big to build models based on it, we will randomly select 100k rows. We will also limit the data to records from 2014 and 2015 (prices may have been changing across the years, hence for prediction we are only interested in the latest years available).

```{r loading data, warning = FALSE, message = FALSE}
#function found on Stack Overflow - 
# https://stackoverflow.com/questions/15532810/reading-40-gb-csv-file-into-r-using-bigmemory

fsample <-
  function(fname, n, seed, header=FALSE, ..., reader = read.csv)
  {
    set.seed(seed)
    con <- file(fname, open="r")
    hdr <- if (header) {
      readLines(con, 1L)
    } else character()
    
    buf <- readLines(con, n)
    n_tot <- length(buf)
    
    repeat {
      txt <- readLines(con, n)
      if ((n_txt <- length(txt)) == 0L)
        break
      
      n_tot <- n_tot + n_txt
      n_keep <- rbinom(1, n_txt, n_txt / n_tot)
      if (n_keep == 0L)
        next
      
      keep <- sample(n_txt, n_keep)
      drop <- sample(n, n_keep)
      buf[drop] <- txt[keep]
    }
    
    reader(textConnection(c(hdr, buf)), header=header, ...)
  }
```

```{r, eval = FALSE}
taxis <- fsample(fname = "train.csv", n = 10^6, seed = 123, header = T, reader = read.csv)

closeAllConnections()

taxis <- taxis %>%
  filter(substr(key, 1, 4) == "2015" | substr(key, 1, 4) == "2014" )

taxis <- taxis[sample(nrow(taxis), 100000), ]

# Let's save the file not to repeat the same computation again - it's quite time consuming
saveRDS(taxis, "taxis_small.rds")
```

```{r, warning = FALSE, message = FALSE}
taxis <- readRDS("taxis_small.rds")
```

Let's inspect the data now. 

There are 8 variables in total. Key is used as in ID of a ride. 'pickup_longitude' and 'pickup_latitude' describe the pickup coordinates. Likewise for dropoff longitude and lattitude. The 'passenger_count' column gives information about the number of passangers in a ride, 'pickup_datetime' for exact time of a pickup and 'fare_amount' about the fare.

```{r}
kable(taxis[1:10, ])
```

There seem to be records that are clearly wrong. It is not possible for fare amount to be negative. Minimum and maximum coordinates are not accurate too. We will focus on filtering wrong data and outliers later.

```{r}
#Confirmation that we indeed have 100 000 observations
nrow(taxis)
summary(taxis)
str(taxis)
```

The average fare for a taxi ride was equal to 12.5\$, while the median was 9.5$. Therefore we can assume that the distribution of the fee is slightly/moderately skewed to the left. Standard deviation is as high as 11.4\$.

```{r}
mean(taxis$fare_amount)
median(taxis$fare_amount)
sd(taxis$fare_amount)
```

There are also no missing values or duplicate rows in the dataset.

```{r}
#do we have NAs
colSums(is.na(taxis)) %>% 
  sort()

#checking for duplicate values
taxis[duplicated(taxis$key),]
```


## Data cleaning and transformations

Since there are no duplicate values, we will delete the 'key' column.
```{r}
#not necessery
taxis$key <- NULL
```

The 'pickup_datetime' column also needs changing to a date format.
```{r}
#changing pickup_datetime to date format
taxis$pickup_datetime <- ymd_hms(taxis$pickup_datetime)
class(taxis$pickup_datetime)
```

Let's inspect the fare amount more. Based on the distribution we can say, that most of the values are between just above 0 and 30\$. There is a slight spike at around 50\$. These observations could be rides to or from an airport which have fixed fees of 52\$. More on that later on.

There are also 23 records with the passenger count of 0. These could be rides that were canceled for some reason before the pickup and the driver had to wait for the client (there is an extra fee for that) or for example the driver was asked to deliver a package to someone. You can see, that the average fee for a ride with 0 passengers is higher, however the sample size is very small. Nevertheless, we can assume that these observations are safe to ignore and therefore we will filter them out. What catches an eye is that there are more rides with 5 passengers than with 3 and 4. This could be related to how many seats there are available in a taxi car.

```{r}
#fare_amount
ggplot(taxis,
       aes(x = fare_amount)) +
  geom_histogram(fill = "lightblue",
                 bins = 100) +
  theme_bw()

ggplot(taxis,
       aes(x = fare_amount)) +
  geom_histogram(fill = "lightblue",
                 bins = 100) +
  theme_bw() + 
  facet_zoom(xlim = c(0, 60))

# boxplot(taxis$fare_amount)

min(taxis$fare_amount)
max(taxis$fare_amount)

#passangers
taxis %>% 
  group_by(passenger_count) %>% 
  summarise(number_of_rides = n(), avg_fee = mean(fare_amount))
```

Therefore we will only keep the fare amount higher than 2.5\$ and lower than 200\$. 2.5\$ is a starting fare for a ride in NYC (based on the information found on the NYC taxi website, please note however, that it's the latest information as of 2022. Prices may have changed during this time, but probably not as significantly. It may be a good idea to try to find price-list from 2014 and 2015 though.  https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page). Moreover, we will filter out rides with 0 passengers.

```{r}
taxis <- taxis %>% 
  filter(fare_amount >= 2.5 & fare_amount < 200 & passenger_count > 0)

#Only 48 observations filtered out. Not many...
nrow(taxis)
```

Additionally, we will also create additional variables - day of the week, month, hour and the year. Moreover, we will add a column with special surcharge depending on the time of the day - night courses and courses during the highest traffic are more expensive (based on the fare table on the NYC taxi website).

```{r}
taxis <- taxis %>% 
  mutate(day = wday(pickup_datetime, label = T),
         month_label = month(pickup_datetime, label = T),
         hour_exact = hour(pickup_datetime),
         year = year(pickup_datetime)) %>%  
  mutate(time_of_day = ifelse(hour_exact >=20 | hour_exact < 6, "overnight", 
                              ifelse(hour_exact >=16 & hour_exact <20 & day %in% c("Mon", "Tue", "Wed", "Thu", "Fri"), "rush", "normal"))
  ) %>% 
  mutate(time_of_day = ifelse(day %in% c("Sat", "Sun"), "overnight", time_of_day))
```

As can be seen on the plot below, the number of rides rises gradually during the week, with most rides on Saturday, and drops significantly on Sunday. 

```{r, warning = FALSE, message = FALSE}
taxis %>% 
  mutate(day = factor(day, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))) %>% 
  ggplot() +
  geom_histogram(aes(x = day), stat = "count", fill = "#69b3a2", alpha = 0.6) +
  theme_minimal() + 
  scale_y_continuous(labels = function(n)format(n, big.mark = " ")) + 
  theme(panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank())
```

We can also notice, that the majority of rides, which may be somewhat surprising, happened during the overnight hours.

```{r, warning = FALSE, message = FALSE}
taxis %>% 
  ggplot(aes(x = round(fare_amount), fill = time_of_day)) +
  geom_histogram(stat = "count", alpha = 0.6) +
  theme_minimal() + 
  scale_y_continuous(labels = function(n)format(n, big.mark = " ")) + 
  theme(panel.grid.major.x = element_blank(), 
        panel.grid.minor.x = element_blank()) +
  facet_zoom(xlim = c(0, 20))
```

We can safely assume, that the fare will be most dependent on the distance of the ride. The standard charge for 1/5 mile in 2022 is equal to 0.50\$. I will therefore calculate the Haversine spatial distance between two coordinates. 

This function outputs distance in meters - hence we will divide it by 1000 to get distance in kilometers.
But first we need to filter incorrect coordinates - the function won't accept them.

```{r}
taxis <- taxis %>%
  filter(pickup_latitude > -90 & pickup_latitude < 90 &
         dropoff_latitude > -90 & dropoff_latitude < 90 & 
         pickup_longitude > -180 & pickup_longitude < 180 &
         dropoff_longitude > -180 & dropoff_longitude < 180) %>% 
  mutate(distance = (distHaversine(cbind(pickup_longitude, pickup_latitude), cbind(dropoff_longitude, dropoff_latitude)))/1000
  )
```

Some of the records have very strange coordinates - we will only use only those near the NYC City. We will select all the coordinates inside the area spanned by points most on the north, south, east and west of NYC (plus a margin of error). It may not be exactly accurate - it may be a better idea to use geojson data of New York boarders - however it would be difficult to implement. Another issue is that some rides could start in New York City but end in one of the neighboring cities. This way we could filter some of the correct observations, however it should only apply to a fraction of observations.

```{r}
taxis <- taxis %>% 
  filter(pickup_longitude < -73 & pickup_longitude > -74.3
         & pickup_latitude < 41.7 & pickup_latitude > 40.5
         & dropoff_longitude < -73 & dropoff_longitude > -74.3
         & dropoff_latitude < 41.7 & dropoff_latitude > 40.5
  )

nrow(taxis)
#Here are many as 2 082 records have been omitted - 2% of all observations
```

In the next step we will check the relationship between the fare and the distance. Based on the plot, it appears that there is a linear relationship between the two. There also seem to be observations with low distance, but a very high fare - it is not entirely impossible that these are rides that started and ended in the same location (example situation: someone wanted to do something quickly in the city center, took a taxi and then returned with the same taxi). It is difficult to judge, hence we will keep these observations, especially that there aren't many of them.

```{r}
taxis %>% 
  ggplot(aes(x = distance, y = fare_amount)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  theme_minimal()

nrow(taxis %>% 
       filter(distance <2 & fare_amount > 40))

# ggplot(taxis, aes(x = distance, y = fare_amount)) + 
#   geom_bin2d(bins = 100) +
#   scale_fill_continuous(type = "viridis") +
#   theme_bw() + xlim (0, 30)
```

We will also try to present pickup locations on the map using leaflet package. Most of he pickups happened in the very center of the city, however, there were also two spots on the south-east and north-east of the city with considerable number of pickups - these are two airports. As can be found on the NYC taxi website - rides to and from the airport can have additional surcharges or even fixed fees. We will therefore add additional variable - stating whether the course started or ended at an airport.

```{r}
geojson = jsonlite::fromJSON("https://services5.arcgis.com/GfwWNkhOj9bNBqoJ/arcgis/rest/services/NYC_Borough_Boundary/FeatureServer/0/query?where=1=1&outFields=*&outSR=4326&f=pgeojson", 
                             simplifyVector = FALSE)

#This one can take a while
leaflet(taxis) %>% 
  addTiles() %>% 
  addCircleMarkers(lat = ~pickup_latitude, lng = ~pickup_longitude, radius = 1, weight = 0.5) %>% 
  setView(lng = -74.00, lat = 40.7128, zoom = 10.5) %>% 
  addGeoJSON(geojson, weight = 0.5)

leaflet(taxis) %>%
  addTiles() %>%
  addHeatmap(lng= ~pickup_longitude, lat = ~pickup_latitude, intensity = 0.5,
             blur = 20, max = 400, radius = 15, cellSize = 3) %>%   
  setView(lng = -74.00, lat = 40.7128, zoom = 12)
```

```{r}
#airports' coordinates
jfk = c(-73.78222, 40.644166)
newark = c(-74.175, 40.69)
la_guardia = c(-73.87, 40.77)

#pickup or dropoff at airport defined as one of those within a distance of 2km from the above coordinate.
#Larger distance seems to result in to many rides not connected to the airport (based on the fare_distribution).

taxis <- taxis %>% 
  mutate(airport = ifelse(
    (distHaversine(cbind(pickup_longitude, pickup_latitude), jfk))<2000 | (distHaversine(cbind(dropoff_longitude, dropoff_latitude), jfk))<2000,
    "JFK", "Not an airport"
  )) %>% 
  mutate(airport = ifelse(
    (distHaversine(cbind(pickup_longitude, pickup_latitude), newark))<2000 | (distHaversine(cbind(dropoff_longitude, dropoff_latitude), newark))<2000,
    "Newark", airport
  )) %>% 
  mutate(airport = ifelse(
    (distHaversine(cbind(pickup_longitude, pickup_latitude), la_guardia))<2000 | (distHaversine(cbind(dropoff_longitude, dropoff_latitude), la_guardia))<2000,
    "La_Guardia", airport
  ))
```

```{r}
#Airports - pickup and drop-off locations (ride could start at an airport and end in a city center and vice versa)
taxis %>% 
  filter(airport %in% c("JFK", "Newark", "La_Guardia")) %>% 
  leaflet() %>% 
  addTiles() %>% 
  addCircleMarkers(lat = ~pickup_latitude, lng = ~pickup_longitude, radius = 1, weight = 0.5) %>% 
  addCircleMarkers(lat = ~dropoff_latitude, lng = ~dropoff_longitude, radius = 1, weight = 0.5, color = "red") %>% 
  setView(lng = -74.00, lat = 40.7128, zoom = 10.5) %>% 
  addGeoJSON(geojson, weight = 0.5)


taxis %>% 
  filter(airport %in% c("JFK", "Newark", "La_Guardia")) %>% 
  ggplot(aes(x = fare_amount)) +
  geom_histogram(fill = "lightblue",
                 bins = 100) +
  ggtitle("Distriution of fare of rides labeled as to or from an airport") + 
  theme_bw()
```


```{r}
# saveRDS(taxis, "taxis_cleaned.rds")
taxis <- readRDS("taxis_cleaned.rds")
```


## Modelling

### Data partition

Firstly, we will divide the dataset into training and testing sets in a proportion of 0.75 and 0.25 respectively.

```{r}
set.seed(123)
training_obs <- createDataPartition(taxis$fare_amount,
                                    p = 0.75,
                                    list = FALSE)
taxis_train <- taxis[training_obs,]
taxis_test  <- taxis[-training_obs,]
```

### Linear regression

First of all, as a benchmark to other models, we are going to use a simple linear regression model. In order to select the variables for the model, we are going to use a stepwise feature selection with Akaikke Information Criterion as a measure for selection.

```{r}
taxis_linear_full <- lm(fare_amount~.,
                   data = taxis_train)

taxis_linear_step <- stepAIC(taxis_linear_full, direction = "both", 
                      trace = FALSE)

summary(taxis_linear_step)
```

```{r}
#function getRegressionMetrics was created by Paweł Sakowski for his Machine Learning 2 classes.
source("C:/Users/marci/Desktop/UW/III Semester/ML2/RF_project/getRegressionMetrics.R")

model_linear_results_train <- getRegressionMetrics(real = taxis_train$fare_amount,
                                             predicted = predict(taxis_linear_step, taxis_train))

model_linear_results_test <- getRegressionMetrics(real = taxis_test$fare_amount,
                     predicted = predict(taxis_linear_step, taxis_test))

model_linear_results_train
model_linear_results_test
```
Root mean squared error (RMSE) on the test data was equal to 4.59\$ and R^2 to 0.83.

We will also inspect the collinearity of variables with the variance inflation factor measure (VIF). Based on Fox & Monette (1992), we will take GVIF^(1/(2*Df)) into consideration and apply to it a standard rule of thumb regarding VIF - that values above 5 indicate a collinearity. Hence, we will assume that there is no such problem in our model, which may be a little bit surprising given that some of the variables, like day, month or hour were extracted from the pickup_datetime variable. 

```{r}
vif(taxis_linear_step)
```

As we earlier showed, the distribution of the independent variable - fare amount, is skewed, therefore we will try to normalize it with a log transformation.

```{r}
#Log transformation
skewness(taxis_train$fare_amount)

taxis_train %>%
  ggplot(aes(x = log(fare_amount))) + 
  geom_histogram(fill = "lightblue",
                 bins = 100) +
  geom_density(alpha = .2, fill = "#FF6666") + 
  theme_minimal()


taxis_linear_log <- lm(log(fare_amount)~.,
                        data = taxis_train)

taxis_linear_step_log <- stepAIC(taxis_linear_log, direction = "both", 
                             trace = FALSE)

summary(taxis_linear_step_log)

getRegressionMetrics(real = log(taxis_test$fare_amount),
                       predicted = predict(taxis_linear_step_log, taxis_test))
```

The adjusted R^2 for a model with a logarithm of fare_amount is however much lower than for previous model (RMSE is different because of different measure - a logarithm). We will thus use the model without normalization. The linear model could be further expanded with with use a Yeo-Johnson transformation instead of log transformation. We will, however, focus more on different models and use linear regression mainly as a benchmark. We have also ommitted the whole diagnostics of a model - which would be necessary if the model was about to be used to make real life predictions about the fare. Based on the below plot, for example, we can suspect a heteroskedasticity to be present.

```{r, warning=FALSE, message=FALSE}
lares::mplot_lineal(tag = taxis_test$fare_amount, 
                    score = predict(taxis_linear_step, taxis_test),
                    subtitle = "Taxi fare regression model",
                    model_name = "Stepwise feature selection")
```

### Regression tree

As the next model for the prediction we will use a regression tree. At first we will use function from the rpart package. 

```{r}
#Decision tree
#sprawdzic, czy na pewno method = anova
set.seed(123)
taxis_tree_rpart <- rpart(fare_amount~.,
                     data = taxis_train,
                     method = "anova")


taxis_tree_rpart

rpart.plot(taxis_tree_rpart)
plotcp(taxis_tree_rpart)

taxis_tree_rpart$cptable

decision_tree_results_rpart_train <- getRegressionMetrics(real = taxis_train$fare_amount,
                                                    predicted = predict(taxis_tree_rpart, taxis_train))

decision_tree_results_rpart_test <- getRegressionMetrics(real = taxis_test$fare_amount,
                                              predicted = predict(taxis_tree_rpart, taxis_test))

decision_tree_results_rpart_train
decision_tree_results_rpart_test
```

For the model, the optimal complexity parameter was selected at 0.01, which resulted in only moderately complex tree and only 6 splits. The complexity parameter of 0.01 seems to be the lowest that is checked by the algorithm. We will therefore try to tune it with the caret package. The accuracy is much lower than for the linear regression - RMSE on the test data is equal to 5.15 and R^2 to 0.78. 

```{r}
tc      <- trainControl(method = "cv", number = 10)
cp.grid <- expand.grid(cp = seq(0, 0.03, 0.001))

set.seed(123)
taxis_tree <- train(fare_amount~.,
                      data = taxis_train, 
                      method = "rpart", 
                      trControl = tc,
                      tuneGrid = cp.grid)

taxis_tree
```

The final model used cp value of (0.001), which resulted in quite a complex tree with 24 splits and 25 terminal nodes. Distance has turned out to be the most important variable in explaining the fare amount, followed by drop-off longitude and the information that the ride is not to or from an airport. For some reason, only the month of November had an importance above 0.

```{r, message=FALSE, warning=FALSE}
fancyRpartPlot(taxis_tree$finalModel, cex = 0.5)
varImp(taxis_tree)
```


```{r}
model_tree_results_train <- getRegressionMetrics(real = taxis_train$fare_amount,
                                                         predicted = predict(taxis_tree, taxis_train))

model_tree_results_test <- getRegressionMetrics(real = taxis_test$fare_amount,
                                                         predicted = predict(taxis_tree, taxis_test))

model_tree_results_train
model_tree_results_test
```

Results of the regression tree with the complexity parameter of 0.001 seem to be much better than of the tree with default cp. RMSE is better than the linear regression's too - it's equal to 4.44 (4.59 for the linear model). It seems to be slightly overfitted however, with RMSE on the train data of 4.35.

### Bagged tree

The next model used in the analysis is the bagged tree - a tree based on a bootstrap aggregation. Bootstrap aggregation, or bagging, for a regression tree is a method in which a sample of data will be selected with a replacement (which means that some observations will be left out completely) and then the results will be averaged over multiple trees build in such a way. To construct a bagged tree we will use bagging() function from *ipred* package. 

We will try to use 100 bootstrap aggregations and the complexity parameter of 0.001 (that is the optimal cp from a single tree from the previous model). It would be interesting to see, however, how would the results look like when the cp parameter was used for each tree seperately.

```{r, eval = FALSE}
set.seed(123)

bag_tree <- bagging(
  formula = fare_amount ~ .,
  data = taxis_train,
  nbagg = 100,
  coob = TRUE,
  control = rpart.control(cp = 0.001)
)

bag_tree
```

```{r, eval = FALSE}
#Saving the results of the model since it takes quite a long to compute it
#The rds file with the model weights as much as 1.5gb...Therefore I'm also going 
#to save the regression metrics, so there is no need to load the model

# saveRDS(bag_tree, "bag_tree.rds")
bag_tree <- readRDS("bag_tree.rds")

model_bagg_results_train <- getRegressionMetrics(real = taxis_train$fare_amount,
                     predicted = predict(bag_tree, taxis_train))
model_bagg_results_test <-getRegressionMetrics(real = taxis_test$fare_amount,
                     predicted = predict(bag_tree, taxis_test))

saveRDS(model_bagg_results_train, "model_bagg_results_train.rds")
saveRDS(model_bagg_results_test, "model_bagg_results_test.rds")
```

```{r}
model_bagg_results_train <- readRDS("model_bagg_results_train.rds")
model_bagg_results_test <- readRDS("model_bagg_results_test.rds")

model_bagg_results_train
model_bagg_results_test
```

Here the results are significantly better than for the previous models. RMSE on the test data is equal to 4.33 and R^2 to 0.85. It's lower than on the train data, which may indicate that the model was slightly overfitted, however the difference is not enormous.

It is also possible to obtain the variable importance from the model, however it is almost exactly the same as for a single regression tree. 

```{r}
VI <- data.frame(var=names(taxis_train[,-1]), imp=varImp(bag_tree))

variables_importance <- data.frame(variable = row.names(varImp(bag_tree)),
                                   importance = varImp(bag_tree)$Overall)

ggplot(data = variables_importance, aes(x = reorder(variable, -importance), y = importance)) +
  geom_bar(stat = "identity", fill = "steelblue2") + 
  labs(x = "Variable") + 
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 70, hjust=1, size = 11))
```
### Random forest

In the next part we are going to build a random forest model. Random forest uses multiple decision trees with additional randomness regarding variables used to construct the tree and sample used. Then, the mean or average prediction of the individual trees is returned.

Firstly, let's use the *randomForest package* to build the model.

```{r, eval = FALSE}
set.seed(123)
taxis_forest <- randomForest(fare_amount~., data = taxis_train, importance = T)

saveRDS(taxis_forest, "taxis_forest.rds")
```

The Random forest was built based on 500 trees and 4 variables tried at each split. However the error seems to be declining only slightly for ntree higher than 100 trees.

```{r}
taxis_forest <- readRDS("taxis_forest.rds")
print(taxis_forest)
plot(taxis_forest)

model_forest_results_train <- getRegressionMetrics(real = taxis_train$fare_amount,
                     predicted = predict(taxis_forest, taxis_train))

model_forest_results_test <- getRegressionMetrics(real = taxis_test$fare_amount,
                     predicted = predict(taxis_forest, taxis_test))

model_forest_results_train
model_forest_results_test
```
RMSE for the random forest was equal to 4.35 and R^2 to 0.85. 

The previous model used 4 variables chosen at random at each split. We will try to optimize this parameter using *caret* package. To reduce the amount of computation, number of trees will be limited to 100, as more resulted in only minor improvement in terms of error.

```{r, eval = FALSE}
parameters <- expand.grid(mtry = 2:9)
ctrl_cv5 <- trainControl(method = "cv", 
                         number = 5)


set.seed(123)
taxis_rf_optimized <-
  train(fare_amount~.,
        data = taxis_train,
        method = "rf",
        ntree = 100,
        # nodesize = 100,
        tuneGrid = parameters,
        trControl = ctrl_cv5,
        importance = TRUE)

saveRDS(object = taxis_rf_optimized, "taxis_rf_optimized.rds")
```

Let's see how this model performed. 

```{r}
taxis_rf_optimized <- readRDS("C:/Users/marci/Desktop/UW/III Semester/ML2/RF_project/taxis_rf_optimized.rds")

getRegressionMetrics(real = taxis_train$fare_amount,
                     predicted = predict(taxis_rf_optimized, taxis_train))

getRegressionMetrics(real = taxis_test$fare_amount,
                     predicted = predict(taxis_rf_optimized, taxis_test))

```
Performance was significantly worse than for the model with default parameters, which is quite surprising. RMSE is 4.44 and, in fact, it is the same as for the single regression tree model. The reason for this could be, perhaps, the limited number of trees (100). We will therefore stay with the default random forest model.

Variable importance seem to coincide with the previous results. A significant difference is, however, the lower importance of information about the airport.

```{r}
varImpPlot(taxis_forest)

variables_importance_rf <- data.frame(variable = row.names(varImp(taxis_forest)),
                                   importance = varImp(taxis_forest)$Overall)

ggplot(data = variables_importance_rf, aes(x = reorder(variable, -importance), y = importance)) +
  geom_bar(stat = "identity", fill = "steelblue2") + 
  labs(x = "Variable") + 
  theme_minimal() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        axis.text.x = element_text(angle = 70, hjust=1, size = 11))
```

### XGBoost

The final model used in the analysis is eXtreme Gradient Boosting (XBGoost) - a popular implementation of gradient boosting methods.

Let's see the parameters for the model. We will try to tune them.
```{r}
modelLookup("xgbTree")
```

At the first step we will try to tune nrounds - number of boosting iterations for the chosen value of learning rate (eta), which we have set to 0.25. Colsample was chosen with the following formula: (sqrt(ncol(train)-1))/(ncol(train)-1) = 0.3. Minimum child weight was selected as 1% of all observations - that is 1 000. Subsample is going to be set at 0.8 - which is a popular starting value. Initial maximum depth is set to 6.

```{r}
parameters_xgb <- expand.grid(nrounds = seq(10, 120, 10),
                              max_depth = c(6),
                              eta = c(0.25), 
                              gamma = 0.5,
                              colsample_bytree = c(0.3),
                              min_child_weight = c(1000),
                              subsample = 0.8)

ctrl_cv5 <- trainControl(method = "cv", number = 5)
```

```{r, warning=FALSE, message=FALSE}
set.seed(123)
taxis_train_xgb <- train(fare_amount~.,
                     data = taxis_train,
                     method = "xgbTree",
                     trControl = ctrl_cv5,
                     tuneGrid  = parameters_xgb)

print(taxis_train_xgb)
```

```{r}
getRegressionMetrics(real = taxis_train$fare_amount,
                     predicted = predict(taxis_train_xgb, taxis_train))

getRegressionMetrics(real = taxis_test$fare_amount,
                     predicted = predict(taxis_train_xgb, taxis_test))
```

Optimal nrounds was selected to 120. In the following steps, we will be tuning other parameters to this value.
Let's first try to tune maximum depth of the tree and minimum child weight. 

```{r, eval = FALSE}
parameters_xgb2 <- expand.grid(nrounds = 120,
                              max_depth = c(5, 6, 7, 8, 9, 10, 11),
                              eta = c(0.25), 
                              gamma = 0.5,
                              colsample_bytree = c(0.3),
                              min_child_weight = seq(400, 1600, 200),
                              subsample = 0.8)

set.seed(123)
taxis_train_xgb2 <- train(fare_amount~.,
                         data = train,
                         method = "xgbTree",
                         trControl = ctrl_cv5,
                         tuneGrid  = parameters_xgb2)

saveRDS(taxis_train_xgb2, "taxis_train_xgb2.rds")
```

```{r}
taxis_train_xgb2 <- readRDS("taxis_train_xgb2.rds")
print(taxis_train_xgb2)
```
So far we have arrived at the following parameters:
nrounds = 120,
max_depth = 11,
colsample_bytree = 0.3,
min_child_weight = 400.

In the net step, we will try to chose the optimal subsample.

```{r, eval = FALSE}
parameters_xgb3 <- expand.grid(nrounds = 120,
                               max_depth = c(11),
                               eta = c(0.25), 
                               gamma = 0.5,
                               colsample_bytree = c(0.3),
                               min_child_weight = c(400),
                               subsample = c(0.6, 0.7, 0.75, 0.8, 0.85, 0.9))

set.seed(123)
taxis_train_xgb3 <- train(fare_amount~.,
                          data = train,
                          method = "xgbTree",
                          trControl = ctrl_cv5,
                          tuneGrid  = parameters_xgb3)


saveRDS(taxis_train_xgb3, "taxis_train_xgb3.rds")
```

```{r}
taxis_train_xgb3 <- readRDS("taxis_train_xgb3.rds")
print(taxis_train_xgb3)
```

Final parameters: 
nrounds = 120, 
max_depth = 8, 
eta = 0.25, 
gamma = 0.5, 
colsample_bytree = 0.3,
min_child_weight = 400,
subsample = 0.9.

```{r}
model_results_xgboost_train <- getRegressionMetrics(real = taxis_train$fare_amount,
                     predicted = predict(taxis_train_xgb3, taxis_train))

model_results_xgboost_test <- getRegressionMetrics(real = taxis_test$fare_amount,
                     predicted = predict(taxis_train_xgb3, taxis_test))

model_results_xgboost_train
model_results_xgboost_test
```

RMSE for the XGBoost model on the test data is equal to 4.38 and R^2, which is comparable to the results of the bagged tree and random forest. 

On the plot below we can see the complexity of the model. For the leaf depth of 9 both number of leafs and cover rises significantly. For our final model, max depth has been selected to 8.

```{r}
#xgb.plot.tree(model = taxis_train_xgb3$finalModel, trees = 1)
xgb.plot.deepness(model = taxis_train_xgb3$finalModel, col = "steelblue")
```

## Models comparison

Finally, let's compare performance of the analyzed models. RMSE fluctuated around 4.4\$, which is only moderately satisfactory, given that the mean of the fare amount was equal to 11.4\$. Based on RMSE, the bagged tree model performed the best, however random forest performed very similarly. Random Forest, however, had lower MAE, Median Absolute Error and Mean Squared Logarithmic Error. All the machine learning models, however, achieved better results (based on RMSE) than simple linear regression model. 

```{r}
results <- cbind(
  data.frame(
    model = c("Linear regression",
    "Random forest",
    "Regression tree",
    "Bagged tree",
    "XGBoost")),
  rbind(
  model_linear_results_test,
  model_forest_results_test,
  model_tree_results_test,
  model_bagg_results_test,
  model_results_xgboost_test)
)

results %>% 
  arrange(RMSE)
```


## Summary

We have shown that machine learning models could be successfully used to predict a taxi fare with a certain level of accuracy. Moreover, for the analysed problem ensamble and gradient boosting methods performed somewhat better than regression tree and linear model. We think that the accuracy of prediction could be further improved by putting more emphasis to the feature engineering and filtering inaccurate data. It may be an interesting idea to find more detailed information about specific surcharges in the analyzed year and try to link given coordinates to additional fees. Additionally, more machine learning methods could be compared. 


###Bibliography

https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/overview
https://www.kaggle.com/tapendrakumar09/xgboost-lgbm-dnn
https://www.scirp.org/(S(lz5mqp453edsnp55rrgjct55.))/reference/referencespapers.aspx?referenceid=2763470
https://xgboost.readthedocs.io/en/stable/
